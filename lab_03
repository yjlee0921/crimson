import numpy as np                           #배열 연산하는 numpy를 임포트하고 np라는 이름을 준다

X = np.array([1, 2, 3])                      #배열 x를 만들어 1, 2, 3 값을 넣어준다
Y = np.array([1, 2, 3])                      #배열 y를 만들어 1, 2, 3 값을 넣어준다

def cost_func(W, X, Y):                      #cost를 계산하는함수
    c = 0                                    #d값을 0으로 저장한다
    for i in range(len(X)):                  #배열 x의 크기만큼 반복한다
        c += (W * X[i] - Y[i]) ** 2          #c에 오차를 제곱한 값을 저장한다 
    return c / len(X)                        #오차의 평균을 구하기 위해 x데이터 갯수로 오차를 나눠준다            

for feed_W in np.linspace(-3, 5, num=15):    #feed_w를 -3~5 사이의 범위를 15개로 나눈값만큼 반복
    curr_cost = cost_func(feed_W, X, Y)      #curr_cost에 cost 함수를 이용해 
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))   #  feed_w | curr_cost 출력
    
#출력값    
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
 #Gradient descent
 
tf.set_random_seed(0)       #random_seed를 0으로 초기화
x_data = [1., 2., 3., 4.]   # x_data [1. , 2. , 3. , 4. ] 입력
y_data = [1., 3., 5., 7.]   # y_data 입력

W = tf.Variable(tf.random_normal([1], -100., 100.))  #정규분포를 따르는 랜덤한 숫자를 만들어 w에 저장

for step in range(300):     #300번 반복한다.
    hypothesis = W * X      #가설함수 H(x) = WX에 값 대입
    cost = tf.reduce_mean(tf.square(hypothesis - Y)) #오차의 평균을 구해 cost에 저장(lab-02에 자세히 설명)

    alpha = 0.01            #alpha 값을 0.01로 설정
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))    #gradient에 X(WX-Y)의 평균을 구해 저장
    descent = W - tf.multiply(alpha, gradient)                           #descent에 W-alpha*gradient 값을 저장
    W.assign(descent)       #새로운 W값 업데이트    
     
    if step % 10 == 0:       #10번에 한번씩
        print('{:5} | {:10.4f} | {:10.6f}'.format(step, cost.numpy(), W.numpy()[0]))    #step| cost | W 출력
        
       0 | 11716.3086 |  48.767971
   10 |  4504.9126 |  30.619968
   20 |  1732.1364 |  19.366755
   30 |   666.0052 |  12.388859
   40 |   256.0785 |   8.062004
   50 |    98.4620 |   5.379007
   60 |    37.8586 |   3.715335
   70 |    14.5566 |   2.683725
   80 |     5.5970 |   2.044044
   90 |     2.1520 |   1.647391
  100 |     0.8275 |   1.401434
  110 |     0.3182 |   1.248922
  120 |     0.1223 |   1.154351
  130 |     0.0470 |   1.095710
  140 |     0.0181 |   1.059348
  150 |     0.0070 |   1.036801
  160 |     0.0027 |   1.022819
  170 |     0.0010 |   1.014150
  180 |     0.0004 |   1.008774
  190 |     0.0002 |   1.005441
  200 |     0.0001 |   1.003374
  210 |     0.0000 |   1.002092
  220 |     0.0000 |   1.001297
  230 |     0.0000 |   1.000804
  240 |     0.0000 |   1.000499
  250 |     0.0000 |   1.000309
  260 |     0.0000 |   1.000192
  270 |     0.0000 |   1.000119
  280 |     0.0000 |   1.000074
  290 |     0.0000 |   1.000046 
